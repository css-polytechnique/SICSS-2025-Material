{"cells":[{"cell_type":"markdown","metadata":{"id":"_r9EedKTflZh"},"source":["## Packages\n","\n","- `pandas` data management\n","- `nltk` old NLP\n","- `matplotlib` vizualisation\n","- `scikit-learn` machine learning"]},{"cell_type":"markdown","metadata":{"id":"i_2ZF2OmflZj"},"source":["## Data\n","\n","Open Alex Computational Social Science"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPO2mMeMflZj"},"outputs":[],"source":["#!pip install pandas nltk scikit-learn matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iO2GV9cNflZk","executionInfo":{"status":"ok","timestamp":1749536474422,"user_tz":-120,"elapsed":1351,"user":{"displayName":"Emilien SCHULTZ","userId":"12248826206444229166"}},"outputId":"4152db3e-059f-4ab0-d71e-8d73c3d1c685"},"outputs":[{"output_type":"stream","name":"stdout","text":["759\n"]},{"output_type":"execute_result","data":{"text/plain":["(690, 182)"]},"metadata":{},"execution_count":1}],"source":["import pandas as pd\n","df = pd.read_csv(\"./sample_data/CSS_exact_openalex.csv\")\n","print(df[\"abstract\"].isna().sum())\n","df = df.dropna(subset=[\"abstract\"])\n","df.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Sa3AaFQflZl","outputId":"1d4fb572-a2e9-4ed1-fdff-8c01d1a838f6"},"outputs":[{"data":{"text/plain":["0       14,0642,033MetricsTotal Downloads14,064Last 6 ...\n","1       The increasing integration of technology into ...\n","3       The integration of social science with compute...\n","7       Abstract Large language models (LLMs) are capa...\n","9       In the first part of the paper, the field of a...\n","                              ...                        \n","1436    Area Studies and the Challenges of Creating a ...\n","1439    Welcome to the third issue of IASSIST Quarterl...\n","1441    My essay has several connected histories to un...\n","1447    Citation (2020), \"Index\", Härtel, C.E.J., Zerb...\n","1448    Citation (2023), \"Index\", Lytras, M.D., Housaw...\n","Name: abstract, Length: 690, dtype: object"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df[\"abstract\"]"]},{"cell_type":"markdown","metadata":{"id":"YCk9oPxJflZl"},"source":["## Cleaning the data"]},{"cell_type":"markdown","metadata":{"id":"DFXtJRzeflZl"},"source":["Some abstract have HTML tags, let's remove them"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MgfR7O3gflZl","outputId":"1e710510-b731-459c-f466-652dc55664a2"},"outputs":[{"data":{"text/plain":["'This is a bold statement.'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","re.sub(r\"<.*?>\", \"\", \"This is a <b>bold</b> statement.\").strip()"]},{"cell_type":"markdown","metadata":{"id":"SgQgBuYVflZl"},"source":["Cleaning function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XC_zApipflZl"},"outputs":[],"source":["def clean_text(text):\n","    \"\"\"\n","    Fonction qui nettoie le texte en supprimant les balises HTML et les espaces inutiles\n","    :param text: le texte à nettoyer\n","    :return: le texte nettoyé\n","    \"\"\"\n","    text = re.sub(r\"<.*?>\", \"\", text)\n","    text = re.sub(r\"\\s\\s+\", \" \", text)\n","    # text = text.capitalize()\n","    return text.strip()"]},{"cell_type":"markdown","metadata":{"id":"bgcooGQIflZl"},"source":["Apply to the corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_2MM6pliflZm","outputId":"8bf877d7-a0f8-4f88-d430-f617aa09c84e"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>abstract</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>14,0642,033MetricsTotal Downloads14,064Last 6 ...</td>\n","      <td>14,0642,033MetricsTotal Downloads14,064Last 6 ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The increasing integration of technology into ...</td>\n","      <td>The increasing integration of technology into ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The integration of social science with compute...</td>\n","      <td>The integration of social science with compute...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Abstract Large language models (LLMs) are capa...</td>\n","      <td>Abstract Large language models (LLMs) are capa...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>In the first part of the paper, the field of a...</td>\n","      <td>In the first part of the paper, the field of a...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  \\\n","0  14,0642,033MetricsTotal Downloads14,064Last 6 ...   \n","1  The increasing integration of technology into ...   \n","3  The integration of social science with compute...   \n","7  Abstract Large language models (LLMs) are capa...   \n","9  In the first part of the paper, the field of a...   \n","\n","                                            abstract  \n","0  14,0642,033MetricsTotal Downloads14,064Last 6 ...  \n","1  The increasing integration of technology into ...  \n","3  The integration of social science with compute...  \n","7  Abstract Large language models (LLMs) are capa...  \n","9  In the first part of the paper, the field of a...  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df[\"text\"] = df[\"abstract\"].apply(clean_text)"]},{"cell_type":"markdown","metadata":{"id":"wfSUBf9ZflZm"},"source":["## Word scale"]},{"cell_type":"markdown","metadata":{"id":"m_iZtiAeflZm"},"source":["### If a word is in the abstract\n","\n","Les bases de la fouille de données. Quels sont les questions qui parlent d'intelligence artificielle ?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_k5l3-v1flZm","outputId":"42219c87-f405-499b-c71a-e944fb19fffb"},"outputs":[{"data":{"text/plain":["np.int64(40)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["filter = df[\"text\"].str.contains(\"LLM\")\n","filter.sum()\n"]},{"cell_type":"markdown","metadata":{"id":"GgxdNm8iflZm"},"source":["### Tokenization"]},{"cell_type":"markdown","metadata":{"id":"3iAPsnYhflZm"},"source":["#### with regex"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aD4TVED5flZm","outputId":"46181e51-2b53-4c36-a579-b34f03419a81"},"outputs":[{"data":{"text/plain":["['This', 'is', 'a', 'test']"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","word_pattern = r\"\\w+\"\n","tokens = re.findall(word_pattern, \"This is a test\")\n","tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zobTwZThflZm","outputId":"089c8c1e-95bd-4401-fa0f-41d0ce4a3c05"},"outputs":[{"data":{"text/plain":["0       [14, 0642, 033metricstotal, downloads14, 064la...\n","1       [the, increasing, integration, of, technology,...\n","3       [the, integration, of, social, science, with, ...\n","7       [abstract, large, language, models, llms, are,...\n","9       [in, the, first, part, of, the, paper, the, fi...\n","                              ...                        \n","1436    [area, studies, and, the, challenges, of, crea...\n","1439    [welcome, to, the, third, issue, of, iassist, ...\n","1441    [my, essay, has, several, connected, histories...\n","1447    [citation, 2020, index, härtel, c, e, j, zerbe...\n","1448    [citation, 2023, index, lytras, m, d, housawi,...\n","Name: text, Length: 690, dtype: object"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["df[\"text\"].apply(lambda x: re.findall(r\"\\w+\",x.lower()))"]},{"cell_type":"markdown","metadata":{"id":"qEneCbYzflZm"},"source":["### with a library `nltk`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwzetcZfflZm","outputId":"b5661d1e-ea95-484c-8e57-2fbb17062e6f"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/emilien/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["['This', 'is', 'a', 'test']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","word_tokenize(\"This is a test\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-e93rNcCflZn","outputId":"86ce21a6-eba5-4788-fa15-c196a1e1041a"},"outputs":[{"data":{"text/plain":["0       [14,0642,033MetricsTotal, Downloads14,064Last,...\n","1       [The, increasing, integration, of, technology,...\n","3       [The, integration, of, social, science, with, ...\n","7       [Abstract, Large, language, models, (, LLMs, )...\n","9       [In, the, first, part, of, the, paper, ,, the,...\n","                              ...                        \n","1436    [Area, Studies, and, the, Challenges, of, Crea...\n","1439    [Welcome, to, the, third, issue, of, IASSIST, ...\n","1441    [My, essay, has, several, connected, histories...\n","1447    [Citation, (, 2020, ), ,, ``, Index, '', ,, Hä...\n","1448    [Citation, (, 2023, ), ,, ``, Index, '', ,, Ly...\n","Name: text, Length: 690, dtype: object"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["df[\"text\"].apply(word_tokenize)"]},{"cell_type":"markdown","metadata":{"id":"m7K-yXSDflZn"},"source":["### Counting the words\n","\n","First with basic tools"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itY7y_2rflZn"},"outputs":[],"source":["from collections import Counter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HakEuDrIflZn","outputId":"e0a3273a-2e67-4a7f-c07a-dd375871026d"},"outputs":[{"data":{"text/plain":["[(',', 9587),\n"," ('the', 6295),\n"," ('of', 5746),\n"," ('and', 5602),\n"," ('.', 5119),\n"," ('to', 3330),\n"," ('in', 2875),\n"," ('a', 2347),\n"," ('social', 1860),\n"," (')', 1459),\n"," ('for', 1447),\n"," ('(', 1439),\n"," ('that', 1281),\n"," ('on', 1257),\n"," ('data', 1145),\n"," ('is', 1058),\n"," ('as', 928),\n"," ('with', 854),\n"," ('science', 852),\n"," ('computational', 838)]"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["counter = Counter([j for i in list(df[\"text\"].apply(word_tokenize)) for j in i])\n","counter.most_common(20)"]},{"cell_type":"markdown","metadata":{"id":"uHhPmMe0flZn"},"source":["Removing the stop words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0oM4JneflZn","outputId":"ff626e27-2dd1-48fd-abfe-308ba5939490"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/emilien/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["['at', 'for', 'up', 'needn', 'this', 'aren', 'while', 'to', \"we've\", 'who']"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download(\"stopwords\")\n","\n","from nltk.corpus import stopwords\n","import string\n","\n","english_stopwords = list(set(stopwords.words(\"english\"))) + list(string.punctuation) + [\"``\", \"''\", \"``\", \"’\", \"“\", \"”\", \"—\", \"–\"]\n","english_stopwords[0:10]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sy5qkb9kflZn","outputId":"17f3b6dc-49e1-4d8f-d0d7-b654946f0ce5"},"outputs":[{"data":{"text/plain":["[('social', 1860),\n"," ('data', 1145),\n"," ('science', 852),\n"," ('computational', 838),\n"," ('research', 723),\n"," ('analysis', 408),\n"," ('media', 402),\n"," ('methods', 337),\n"," ('study', 315),\n"," ('information', 299),\n"," ('models', 288),\n"," ('new', 287),\n"," ('digital', 287),\n"," ('Social', 263),\n"," ('work', 256),\n"," ('also', 254),\n"," ('use', 252),\n"," ('using', 251),\n"," ('model', 235),\n"," ('political', 229)]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["counter = Counter([j for i in list(df[\"text\"].apply(word_tokenize)) for j in i if j.lower() not in english_stopwords])\n","counter.most_common(20)"]},{"cell_type":"markdown","metadata":{"id":"v9GHtWUQflZn"},"source":["### Most frequent words combinations\n","\n","Bigrams and trigrams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32sh2MXoflZn","outputId":"857339f7-ffc1-428c-8c76-b869118df679"},"outputs":[{"data":{"text/plain":["[('increasing', 'integration'),\n"," ('integration', 'technology'),\n"," ('technology', 'lives'),\n"," ('lives', 'created'),\n"," ('created', 'unprecedented'),\n"," ('unprecedented', 'volumes'),\n"," ('volumes', 'data'),\n"," ('data', 'society'),\n"," ('society', \"'s\"),\n"," (\"'s\", 'everyday')]"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.util import ngrams\n","from nltk.tokenize import word_tokenize\n","\n","def generate_bigrams_nltk(text):\n","    tokens = word_tokenize(text.lower())\n","    tokens = [token for token in tokens if token not in english_stopwords]\n","    bigrams = list(ngrams(tokens, 2))\n","    return bigrams\n","\n","generate_bigrams_nltk(df[\"text\"].iloc[1])[0:10]"]},{"cell_type":"markdown","metadata":{"id":"80xQBuuGflZo"},"source":["Couting them"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qPeuInGflZo","outputId":"806a46fe-0f7c-4db7-b280-d774dc3d2b61"},"outputs":[{"data":{"text/plain":["[(('social', 'science'), 737),\n"," (('computational', 'social'), 730),\n"," (('social', 'media'), 297),\n"," (('social', 'sciences'), 173),\n"," (('big', 'data'), 129),\n"," (('machine', 'learning'), 85),\n"," (('data', 'science'), 62),\n"," (('computational', 'methods'), 60),\n"," (('social', 'networks'), 58),\n"," (('science', 'research'), 58),\n"," (('natural', 'language'), 55),\n"," (('social', 'network'), 52),\n"," (('language', 'processing'), 50),\n"," (('large', 'language'), 47),\n"," (('language', 'models'), 47),\n"," (('field', 'computational'), 46),\n"," (('science', 'css'), 44),\n"," (('social', 'scientists'), 44),\n"," (('media', 'data'), 43),\n"," (('smart', 'cities'), 41)]"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["counter = Counter([j for i in list(df[\"text\"].apply(generate_bigrams_nltk)) for j in i])\n","counter.most_common(20)"]},{"cell_type":"markdown","metadata":{"id":"s_z89FRaflZo"},"source":["## Representing texts"]},{"cell_type":"markdown","metadata":{"id":"sKCVbgFVflZo"},"source":["### Manually"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A50OBAuPflZo","outputId":"03067251-dba9-499d-8608-9d1cf402b06d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>dim1</th>\n","      <th>dim2</th>\n","      <th>dim3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1436</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1439</th>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1441</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1447</th>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1448</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>690 rows × 3 columns</p>\n","</div>"],"text/plain":["       dim1   dim2   dim3\n","0     False  False  False\n","1     False  False  False\n","3     False  False  False\n","7      True  False  False\n","9     False  False  False\n","...     ...    ...    ...\n","1436  False  False  False\n","1439  False   True  False\n","1441  False  False   True\n","1447  False   True   True\n","1448  False  False  False\n","\n","[690 rows x 3 columns]"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["df[\"dim1\"] = df[\"text\"].str.contains(\"LLM\")\n","df[\"dim2\"] = df[\"text\"].str.contains(\"IA\")\n","df[\"dim3\"] = df[\"text\"].str.contains(\"algorithm\")\n","df[[\"dim1\", \"dim2\", \"dim3\"]]"]},{"cell_type":"markdown","metadata":{"id":"ccUJu6B6flZo"},"source":["### Using `scikit-learn` to create the DTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVxgGixqflZo","outputId":"be57ba53-e253-40ad-899c-107eca795603"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>10</th>\n","      <th>19</th>\n","      <th>2020</th>\n","      <th>abstract</th>\n","      <th>across</th>\n","      <th>address</th>\n","      <th>age</th>\n","      <th>agent</th>\n","      <th>ai</th>\n","      <th>al</th>\n","      <th>...</th>\n","      <th>web</th>\n","      <th>well</th>\n","      <th>within</th>\n","      <th>without</th>\n","      <th>word</th>\n","      <th>words</th>\n","      <th>work</th>\n","      <th>world</th>\n","      <th>years</th>\n","      <th>yet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 300 columns</p>\n","</div>"],"text/plain":["   10  19  2020  abstract  across  address  age  agent  ai  al  ...  web  \\\n","0   0   0     0         0       0        0    0      0   0   0  ...    0   \n","1   0   0     0         0       0        0    0      0   0   0  ...    0   \n","2   0   0     0         0       0        0    0      0   0   0  ...    0   \n","3   0   0     0         1       0        0    0      0   0   0  ...    0   \n","4   0   0     0         0       0        0    0      1   0   0  ...    0   \n","\n","   well  within  without  word  words  work  world  years  yet  \n","0     0       0        0     0      0     0      0      0    0  \n","1     0       1        0     0      0     1      0      0    0  \n","2     0       1        0     0      0     1      0      0    0  \n","3     0       0        1     0      0     1      0      0    0  \n","4     0       0        0     0      0     0      0      0    0  \n","\n","[5 rows x 300 columns]"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(stop_words=english_stopwords, ngram_range=(1, 1), max_features=300)\n","\n","dfm = vectorizer.fit_transform(df[\"text\"])\n","\n","# shape it\n","dtm = pd.DataFrame(\n","        dfm.toarray(),\n","        columns=vectorizer.get_feature_names_out(),\n","    )\n","\n","dtm.head()"]},{"cell_type":"markdown","metadata":{"id":"mQ-QQ0YVflZo"},"source":["### More advanced version\n","\n","- Term Frequency-Inverse Document Frequency\n","\n","$$\\text{TF-IDF}(t, d, D) = \\left( \\frac{f_{t,d}}{n_d} \\right) \\times \\log \\left(\\frac{N}{\\text{df}_t} \\right)\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ETQyK5KflZo","outputId":"a40f6f63-d94c-41c0-b719-c0ff37b48449"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>10</th>\n","      <th>19</th>\n","      <th>2020</th>\n","      <th>abstract</th>\n","      <th>across</th>\n","      <th>address</th>\n","      <th>age</th>\n","      <th>agent</th>\n","      <th>ai</th>\n","      <th>al</th>\n","      <th>...</th>\n","      <th>web</th>\n","      <th>well</th>\n","      <th>within</th>\n","      <th>without</th>\n","      <th>word</th>\n","      <th>words</th>\n","      <th>work</th>\n","      <th>world</th>\n","      <th>years</th>\n","      <th>yet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.145328</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.126847</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.184266</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.160833</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.077291</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.079879</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.058495</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.199179</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 300 columns</p>\n","</div>"],"text/plain":["    10   19  2020  abstract  across  address  age     agent   ai   al  ...  \\\n","0  0.0  0.0   0.0  0.000000     0.0      0.0  0.0  0.000000  0.0  0.0  ...   \n","1  0.0  0.0   0.0  0.000000     0.0      0.0  0.0  0.000000  0.0  0.0  ...   \n","2  0.0  0.0   0.0  0.000000     0.0      0.0  0.0  0.000000  0.0  0.0  ...   \n","3  0.0  0.0   0.0  0.077291     0.0      0.0  0.0  0.000000  0.0  0.0  ...   \n","4  0.0  0.0   0.0  0.000000     0.0      0.0  0.0  0.199179  0.0  0.0  ...   \n","\n","   web  well    within   without  word  words      work  world  years  yet  \n","0  0.0   0.0  0.000000  0.000000   0.0    0.0  0.000000    0.0    0.0  0.0  \n","1  0.0   0.0  0.145328  0.000000   0.0    0.0  0.126847    0.0    0.0  0.0  \n","2  0.0   0.0  0.184266  0.000000   0.0    0.0  0.160833    0.0    0.0  0.0  \n","3  0.0   0.0  0.000000  0.079879   0.0    0.0  0.058495    0.0    0.0  0.0  \n","4  0.0   0.0  0.000000  0.000000   0.0    0.0  0.000000    0.0    0.0  0.0  \n","\n","[5 rows x 300 columns]"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# créer un objet\n","vectorizer = TfidfVectorizer(stop_words=english_stopwords,\n","                             ngram_range=(1, 1),\n","                             max_features=300)\n","\n","# applique\n","X = vectorizer.fit_transform(df[\"text\"])\n","\n","# mettre en forme\n","X = pd.DataFrame(X.toarray(),columns=list(vectorizer.get_feature_names_out()))\n","X.head()"]},{"cell_type":"markdown","metadata":{"id":"fcUBA4H0flZ1"},"source":["Most important words for a document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VPzIiyHcflZ1","outputId":"6b835062-d99d-4a3b-ed09-9858a8b08771"},"outputs":[{"data":{"text/plain":["'data'"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["X.loc[12].idxmax()"]},{"cell_type":"markdown","metadata":{"id":"1bAZT87hflZ1"},"source":["## Distance between texts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ESAxI7pflZ1","outputId":"10909bc8-9620-4967-bc64-9c52c1186b23"},"outputs":[{"data":{"text/plain":["array([[0.]])"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.metrics import pairwise_distances\n","\n","X = vectorizer.fit_transform(df[\"text\"])\n","cosine_similarity(X[0], X[100])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcPnoNwoflZ1"},"outputs":[],"source":["distances = pd.DataFrame(pairwise_distances(X, metric=\"cosine\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJP4z3C1flZ1","outputId":"f255c193-62dd-406a-e48e-91b7b49e2a7b"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["distances[10].idxmax()"]},{"cell_type":"markdown","metadata":{"id":"F-dxhLPTflZ1"},"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[{"file_id":"1HyXkrJAXjIINbfCvcMQ96VfB1a41ENSN","timestamp":1750684595722}]}},"nbformat":4,"nbformat_minor":0}